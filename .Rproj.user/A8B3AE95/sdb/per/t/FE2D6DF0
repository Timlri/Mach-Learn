{
    "contents" : "---\ntitle: \"Assessment of Exercise Quality\"\nauthor: \"Tim Reid\"\ndate: \"Sunday, January 25, 2015\"\noutput: html_document\n---\nWe are given a set of measurement data from accelerometers attached to 6 subjects during the performance of an exercise routine, and asked to create a model to assess the quality of their performance of that exercise. The measurement data is captured in a set of 19,622 observations of 160 variables, and includes a <classe> variable that characterizes the quality of the exercise performance.\n\nAn additional testing data set is provided to test the model.\n\nPreliminary Data Exploration\n----------------------------\n\nFirst, access needed libraries and load the training data set.\n\n```{r, echo=TRUE}\nlibrary(lattice)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(rpart)\nlibrary(MASS)\nlibrary(corrplot)\n\npml.training <- read.csv(\"~/Spectrum-R/Coursera/MachineLearning/pml-training.csv\", \n              header = TRUE, stringsAsFactors = TRUE)\n```\n\nFrom a \"big picture\" view of the data we see that there are many missing values. Many of the predictors are nearly devoid of values. In the chart below the light colored areas reflect missing values.\n\n```{r, echo=FALSE}\nimage(is.na(pml.training), main = \"Missing Values\", xlab = \"Observation\", \n      ylab = \"Predictor\", xaxt = \"n\", yaxt = \"n\", bty = \"n\")\naxis(1, seq(0, 1, length.out = nrow(pml.training)), 1:nrow(pml.training), col = \"white\")\naxis(2, seq(0, 1, length.out = ncol(pml.training)), 1:ncol(pml.training), tick = FALSE)\n```\n\nExamining the data set in detail, it appears that this is a set of measurements taken rapidly over a short time interval. The measurements are frequently summarized. Some predictors are only populated during the summaries. \n\nOne solution is to select a subset of predictors for which we are provided complete data. With only a little knowledge of the data set we can select a subset containing those predictors and eliminate others (user_name, timestamps, etc.) that may confound the results.\n\n```{r, echo=TRUE}\npml.training <- pml.training[ , c(8:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)]\ndim(pml.training)\n```\n\nThis leaves 19,622 observations of 53 variables, including the CLASSE response variable.\n\nPreprocessing\n-------------\n\nNext we divide the pml-training set into a training and a testing set that we can later use to cross validate the models.\n\n```{r, echo=TRUE}\ntr.index <- createDataPartition(y = pml.training$classe, p = 0.80, list = FALSE)\ntraining <- pml.training[ tr.index, ]\ntesting  <- pml.training[ -tr.index, ]\n```\n\nUsing the tools in the caret package we next look for near zero-variance predictors which we might wish to eliminate as they may create problems for some model types.\n\n```{r, echo=TRUE}\nnearZeroVar(training, saveMetrics= FALSE)\n```\n\nThe nearZeroVar function returns a null result indicating that there are no near zero-variance variables among the remaining predictors.\n\nWe next examine correlations among the predictors, and remove predictors that have a pairwise correlation greater than 0.80.\n\n```{r, echo=TRUE}\nset.seed(1)\nM <- cor(training[ , -length(training)])\ncorrplot(M, order = \"original\", tl.pos = \"n\")\nhCor <- findCorrelation(M, 0.80)\ntraining <- training[, -hCor]\ntesting  <- testing[, -hCor]\n```\n\nThis leaves \n\n```{r, echo=TRUE}\ndim(training)[2]\n```\n\npredictors.\n\nRunning the Models\n------------------\n\nWe'll first pre-process using principal components analysis.\n\n```{r, echo=TRUE}\nset.seed(8181)\npreProc <- preProcess(training[, -length(training)], method = \"pca\")\ntrainPC <- predict(preProc, training[, -length(training)])\n```\n\nThis leaves \n\n```{r, echo=TRUE}\ndim(trainPC)[2]\n```\n\npredictors.\n\nSo, we begin with a recursive partitioning model from the rpart package.\n\n```{r, echo=TRUE}\nset.seed(8181)\nrp1 <- train(training$classe ~ ., data = trainPC, method=\"rpart\")\n```\n\nThe accuracy is disappointing.\n\n```{r, echo = TRUE}\nrp1$results[1,2]\n```\n\nNext we try a k-nearest neighbor model.\n\n```{r, echo=TRUE}\nset.seed(8181)\nrp2 <- train(training$classe ~ ., data=trainPC, method=\"knn\")\n```\n\nThe accuracy here is promising. The model accuracy is\n\n```{r, echo = TRUE}\nrp2$results[1,2]\n```\n\nNow we evaluate the fit of the knn model using the cross-validation set. An examination of the confusion matrix shows \n\n```{r, echo = TRUE}\ntestPC <- predict(preProc, testing[, -length(testing)])\ncf <- confusionMatrix(testing$classe,predict(rp2, newdata = testPC))\ncf\n```\n\nAn examination of the confusion matrix shows that the out-of-sample accuracy is \n\n```{r, echo = FALSE}\ncf$overall[1]\n```\n\nwhich is very good.\n\nSummary\n-------\n\nFinally, we turn to the pml-testing data provided to \"grade\" the 20 test samples using our model.\n\n```{r, echo = TRUE}\npml.testing  <- read.csv(\"~/Spectrum-R/Coursera/MachineLearning/pml-testing.csv\", \n                     header = TRUE, stringsAsFactors = TRUE)\npml.testing  <- pml.testing[ ,  c(8:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160)]\npml.testing  <- pml.testing[, -hCor]\n\ntPC <- predict(preProc, pml.testing[, -length(pml.testing)])\nresult.set <- predict(rp2$finalModel, tPC, type = \"class\")\nresult.set\n```\n",
    "created" : 1422191818701.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2261104076",
    "id" : "FE2D6DF0",
    "lastKnownWriteTime" : 1422191850,
    "path" : "~/Spectrum-R/Coursera/MachineLearning/Index.Rmd",
    "project_path" : "Index.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}